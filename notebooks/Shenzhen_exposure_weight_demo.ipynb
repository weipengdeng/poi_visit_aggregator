{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3fe38b7",
   "metadata": {},
   "source": [
    "# Shenzhen exposure-weight demo (grid / H3 hex)\n",
    "\n",
    "This notebook computes **user ? spatial-unit exposure weights** (grid or H3 hex) in lunch/dinner windows using `poi_visit_aggregator`.\n",
    "\n",
    "Outputs (per city):\n",
    "- grid: `user_grid_time_strict_filled_<city>.parquet` + `qa_summary_strict_filled_<city>.csv`\n",
    "- hex: `user_hex_time_strict_filled_<city>.parquet` + `qa_summary_hex_strict_filled_<city>.csv`\n",
    "\n",
    "Two common workflows:\n",
    "1) **Visualize existing outputs (recommended)**: set `EXPO_DIR` to the folder that contains the two output files.\n",
    "2) **Run export (slow)**: set `RUN_EXPORT=True` and configure `STAYPOINTS/UUID_TABLE` plus `GRID_META` (grid) or `HEX_META` (hex).\n",
    "\n",
    "Tip: for stability/performance, put `tmp_root` and DuckDB temp on a local disk (Colab: `/tmp`, Windows: e.g. `C:\\temp`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf85115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Colab detection ---\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "\n",
    "def _find_repo_root(start: Path) -> Path:\n",
    "    p = start.resolve()\n",
    "    for parent in [p, *p.parents]:\n",
    "        if (parent / \"pyproject.toml\").exists() and (parent / \"poi_visit_aggregator\").exists():\n",
    "            return parent\n",
    "    return start.resolve()\n",
    "\n",
    "\n",
    "# --- Colab bootstrap (Drive + optional clone) ---\n",
    "# If you open this notebook directly in Colab (not opened from the repo),\n",
    "# keep CLONE_REPO_IN_COLAB=True to clone the repo into Drive and add it to sys.path.\n",
    "CLONE_REPO_IN_COLAB = True\n",
    "COLAB_TARGET_DIR = Path(\"/content/drive/MyDrive/Script/Module\")\n",
    "COLAB_REPO_PATH = COLAB_TARGET_DIR / \"poi_visit_aggregator\"\n",
    "\n",
    "if IN_COLAB and CLONE_REPO_IN_COLAB:\n",
    "    drive.mount(\"/content/drive\")\n",
    "    COLAB_TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    os.chdir(str(COLAB_TARGET_DIR))\n",
    "\n",
    "    if not COLAB_REPO_PATH.exists():\n",
    "        get_ipython().system(\"git clone https://github.com/weipengdeng/poi_visit_aggregator.git\")\n",
    "    else:\n",
    "        try:\n",
    "            get_ipython().system(f\"git -C {COLAB_REPO_PATH} pull\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if str(COLAB_REPO_PATH) in sys.path:\n",
    "        sys.path.remove(str(COLAB_REPO_PATH))\n",
    "    sys.path.insert(0, str(COLAB_REPO_PATH))\n",
    "    os.chdir(str(COLAB_REPO_PATH))\n",
    "\n",
    "if not IN_COLAB:\n",
    "    # VSCode/Jupyter sometimes sets CWD to `notebooks/`. Make repo imports work either way.\n",
    "    REPO_ROOT = _find_repo_root(Path.cwd())\n",
    "    os.chdir(str(REPO_ROOT))\n",
    "    if str(REPO_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "try:\n",
    "    import psutil  # type: ignore\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "\n",
    "def mem_gb():\n",
    "    \"\"\"Return (rss_gb, avail_gb, used_pct).\"\"\"\n",
    "    if psutil is None:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    vm = psutil.virtual_memory()\n",
    "    rss = psutil.Process(os.getpid()).memory_info().rss\n",
    "    return (float(rss) / 1e9, float(vm.available) / 1e9, float(vm.percent))\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "print(\"IN_COLAB:\", IN_COLAB)\n",
    "print(\"CWD:\", Path.cwd())\n",
    "rss_gb, avail_gb, used_pct = mem_gb()\n",
    "print(f\"Memory: RSS={rss_gb:.2f} GB | avail={avail_gb:.2f} GB | used={used_pct:.0f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2218339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install export deps (run once)\n",
    "# If you rerun the notebook often, you can comment this cell after the first run.\n",
    "#\n",
    "# Colab:\n",
    "# !pip -q install -e \".[export]\"\n",
    "#\n",
    "# Local (VSCode/Jupyter):\n",
    "# !{sys.executable} -m pip install -e \".[export]\"\n",
    "#\n",
    "# Optional (viz):\n",
    "# !{sys.executable} -m pip install plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e76ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "CITY = \"Shenzhen\"\n",
    "# City code that matches staypoints `c_code`.\n",
    "GRID_UID_CODE = \"440300\"  # used when UNIT_MODE='grid'\n",
    "HEX_UID_CODE = \"440300\"   # used when UNIT_MODE='hex'\n",
    "\n",
    "# Spatial unit: 'grid' (default) or 'hex' (H3).\n",
    "UNIT_MODE = os.environ.get(\"POI_VISIT_UNIT_MODE\", \"hex\").strip().lower()\n",
    "if UNIT_MODE not in {\"grid\", \"hex\"}:\n",
    "    raise ValueError(\"POI_VISIT_UNIT_MODE must be grid|hex\")\n",
    "UNIT_UID_COL = \"grid_uid\" if UNIT_MODE == \"grid\" else \"hex_uid\"\n",
    "\n",
    "# If you already have the outputs, keep RUN_EXPORT=False and set EXPO_DIR.\n",
    "RUN_EXPORT = False\n",
    "\n",
    "# --- Outputs location (visualize-only) ---\n",
    "if IN_COLAB:\n",
    "    DRIVE_ROOT = Path(\"/content/drive/MyDrive\")\n",
    "    EXPO_DIR = DRIVE_ROOT / \"Project/202512_EFE/data/expo\"\n",
    "else:\n",
    "    EXPO_DIR = Path(os.environ.get(\"POI_VISIT_EXPO_DIR\", r\"E:\\\\OneDrive\\\\HKU\\\\Project\\\\202512_EFE\\\\data\\\\expo\"))\n",
    "\n",
    "OUT_CITY_DIR = EXPO_DIR\n",
    "OUT_FILE = OUT_CITY_DIR / (\n",
    "    f\"user_grid_time_strict_filled_{CITY}.parquet\"\n",
    "    if UNIT_MODE == \"grid\"\n",
    "    else f\"user_hex_time_strict_filled_{CITY}.parquet\"\n",
    ")\n",
    "QA_FILE = OUT_CITY_DIR / (\n",
    "    f\"qa_summary_strict_filled_{CITY}.csv\" if UNIT_MODE == \"grid\" else f\"qa_summary_hex_strict_filled_{CITY}.csv\"\n",
    ")\n",
    "\n",
    "# --- Grid meta for map visualization (grid only) ---\n",
    "GRID_META_VIZ = None\n",
    "if UNIT_MODE == \"grid\":\n",
    "    GRID_META_VIZ = os.environ.get(\"POI_VISIT_GRID_META_VIZ\", \"\").strip()\n",
    "    if GRID_META_VIZ:\n",
    "        GRID_META_VIZ = Path(GRID_META_VIZ)\n",
    "\n",
    "# --- Hex meta (H3) ---\n",
    "HEX_META = os.environ.get(\"POI_VISIT_HEX_META\", \"\").strip()\n",
    "if HEX_META:\n",
    "    HEX_META = Path(HEX_META)\n",
    "else:\n",
    "    default_hex = Path.cwd() / f\"hex_meta_{CITY}_{HEX_UID_CODE}.json\"\n",
    "    HEX_META = default_hex if default_hex.exists() else None\n",
    "\n",
    "# --- (Optional) run export in this notebook ---\n",
    "if RUN_EXPORT:\n",
    "    if IN_COLAB:\n",
    "        DRIVE_ROOT = Path(\"/content/drive/MyDrive\")\n",
    "        DATA_ROOT = DRIVE_ROOT / \"Project/202512_EFE\"\n",
    "\n",
    "        UUID_TABLE = DATA_ROOT / \"data/jike/uuid.csv\"  # .csv or .parquet\n",
    "        STAYPOINTS = [DATA_ROOT / \"data/jike/track.csv\"]\n",
    "\n",
    "        GRID_META = DATA_ROOT / f\"grid_meta_{CITY}.json\"\n",
    "        HEX_META = DATA_ROOT / f\"hex_meta_{CITY}_{HEX_UID_CODE}.json\"\n",
    "\n",
    "        OUT_DIR = DATA_ROOT / \"out/poi_visit_aggregator\"\n",
    "\n",
    "        # Recommended: keep temp files off Google Drive.\n",
    "        TMP_ROOT = Path(\"/tmp/poi_visit_aggregator_tmp\")\n",
    "        DUCKDB_TEMP_DIR = Path(\"/tmp/duckdb_tmp\")\n",
    "    else:\n",
    "        # TODO: edit these paths to your local disk (raw inputs).\n",
    "        DATA_ROOT = Path(r\"D:\\\\Project\\\\202512_EFE\")\n",
    "        UUID_TABLE = DATA_ROOT / \"uuid.csv\"  # .csv or .parquet\n",
    "        STAYPOINTS = [DATA_ROOT / \"staypoints.csv\"]\n",
    "\n",
    "        GRID_META = DATA_ROOT / f\"grid_meta_{CITY}.json\"\n",
    "        HEX_META = DATA_ROOT / f\"hex_meta_{CITY}_{HEX_UID_CODE}.json\"\n",
    "\n",
    "        OUT_DIR = DATA_ROOT / \"out/poi_visit_aggregator\"\n",
    "\n",
    "        # Put temp on a fast local disk (avoid OneDrive folders).\n",
    "        temp_dir = Path(os.environ.get(\"TEMP\", \".\"))\n",
    "        TMP_ROOT = Path(os.environ.get(\"POI_VISIT_TMP_ROOT\", str(temp_dir / \"poi_visit_aggregator_tmp\")))\n",
    "        DUCKDB_TEMP_DIR = Path(os.environ.get(\"POI_VISIT_DUCKDB_TMP\", str(temp_dir / \"duckdb_tmp\")))\n",
    "\n",
    "    # If staypoints contain nationwide users, keep this on for speed.\n",
    "    FILTER_CITY_CODE = True\n",
    "    CITY_CODE_COL = \"c_code\"\n",
    "    CITY_CODE_VALUE = GRID_UID_CODE if UNIT_MODE == \"grid\" else HEX_UID_CODE\n",
    "\n",
    "    OUT_CITY_DIR = OUT_DIR / CITY\n",
    "    OUT_FILE = OUT_CITY_DIR / (\n",
    "        f\"user_grid_time_strict_filled_{CITY}.parquet\"\n",
    "        if UNIT_MODE == \"grid\"\n",
    "        else f\"user_hex_time_strict_filled_{CITY}.parquet\"\n",
    "    )\n",
    "    QA_FILE = OUT_CITY_DIR / (\n",
    "        f\"qa_summary_strict_filled_{CITY}.csv\"\n",
    "        if UNIT_MODE == \"grid\"\n",
    "        else f\"qa_summary_hex_strict_filled_{CITY}.csv\"\n",
    "    )\n",
    "\n",
    "print(\"UNIT_MODE:\", UNIT_MODE)\n",
    "print(\"UNIT_UID_COL:\", UNIT_UID_COL)\n",
    "print(\"OUT_FILE:\", OUT_FILE)\n",
    "print(\"QA_FILE:\", QA_FILE)\n",
    "print(\"GRID_META_VIZ:\", GRID_META_VIZ)\n",
    "print(\"HEX_META:\", HEX_META)\n",
    "\n",
    "if UNIT_MODE == \"hex\" and HEX_META is not None and HEX_META.exists():\n",
    "    meta = json.loads(HEX_META.read_text(encoding=\"utf-8-sig\"))\n",
    "    print(\"[hex_meta] city=\", meta.get(\"city\"), \"code=\", meta.get(\"code\"), \"res=\", meta.get(\"h3_resolution\"), \"n=\", meta.get(\"n_hexes\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ba0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: map your column names if they differ.\n",
    "# Fill in only what you need.\n",
    "SCHEMA_MAP = {\n",
    "    \"staypoints\": {\n",
    "        # \"uuid\": \"uuid\",\n",
    "        # \"start_time\": \"start_ms\",\n",
    "        # \"end_time\": \"end_ms\",\n",
    "        # one of (x,y) or (lon,lat) or location\n",
    "        # \"lon\": \"lon\",\n",
    "        # \"lat\": \"lat\",\n",
    "        # \"location\": \"location\",\n",
    "        # \"source\": \"source\",\n",
    "        # \"c_code\": \"c_code\",\n",
    "    },\n",
    "    \"uuid_table\": {\n",
    "        # \"uuid\": \"uuid\",\n",
    "    },\n",
    "}\n",
    "\n",
    "SCHEMA_MAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bb59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "try:\n",
    "    import psutil  # type: ignore\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "\n",
    "def rss_mb() -> float:\n",
    "    if psutil is None:\n",
    "        return float(\"nan\")\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def step(name: str):\n",
    "    t0 = time.perf_counter()\n",
    "    m0 = rss_mb()\n",
    "    print(f\"[START] {name} (RAM={m0:,.0f} MB)\")\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        dt = time.perf_counter() - t0\n",
    "        m1 = rss_mb()\n",
    "        print(f\"[ END ] {name} (dt={dt:,.1f}s, RAM={m1:,.0f} MB, ?={m1 - m0:,.0f} MB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "RESUME_STAGE2 = False  # set True to reuse existing Stage-1 parts\n",
    "KEEP_INTERMEDIATE = True  # keep Stage-1 parts (useful for resume/debug)\n",
    "\n",
    "if RUN_EXPORT:\n",
    "    if UNIT_MODE == \"grid\":\n",
    "        from poi_visit_aggregator.export_user_grid_time_strict_filled import (\n",
    "            export_user_grid_time_strict_filled,\n",
    "        )\n",
    "\n",
    "        with step(\"export_user_grid_time_strict_filled\"):\n",
    "            export_user_grid_time_strict_filled(\n",
    "                city=CITY,\n",
    "                staypoints=[Path(p) for p in STAYPOINTS],\n",
    "                staypoints_format=\"auto\",  # or csv/parquet\n",
    "                uuid_table=UUID_TABLE,\n",
    "                grid_meta_path=GRID_META,\n",
    "                out_dir=OUT_DIR,\n",
    "                tmp_root=TMP_ROOT,\n",
    "                duckdb_temp_dir=DUCKDB_TEMP_DIR,\n",
    "                schema_map=SCHEMA_MAP,\n",
    "                output_grid_uid=True,\n",
    "                output_grid_id=False,\n",
    "                grid_uid_code=GRID_UID_CODE,\n",
    "                grid_uid_prefix=\"grid\",\n",
    "                grid_uid_order=\"col_row\",\n",
    "                filter_city_code=FILTER_CITY_CODE,\n",
    "                city_code_col=CITY_CODE_COL,\n",
    "                city_code_value=CITY_CODE_VALUE,\n",
    "                windows=[\"lunch\", \"dinner\"],\n",
    "                min_interval_minutes=5,\n",
    "                point_source_filter=True,\n",
    "                point_source_value=\"cell_appearance\",\n",
    "                drop_uuid_not_in_table=True,\n",
    "                timestamps_are_utc=True,\n",
    "                tz_offset_hours=8,\n",
    "                epoch_unit=\"ms\",\n",
    "                coords_already_projected=False,\n",
    "                uid64_hash_method=\"xxh64\",\n",
    "                buckets=256,\n",
    "                batch_size=1_000_000,\n",
    "                log_every_batches=500,\n",
    "                overlap_rounding=\"floor\",\n",
    "                oob_mode=\"drop\",\n",
    "                threads=max(1, (os.cpu_count() or 8) - 1),\n",
    "                memory_limit=\"8GB\",\n",
    "                id_mode=\"uuid\",  # uuid|uid64|both\n",
    "                resume_stage2=RESUME_STAGE2,\n",
    "                keep_intermediate=KEEP_INTERMEDIATE,\n",
    "            )\n",
    "    else:\n",
    "        from poi_visit_aggregator.export_user_hex_time_strict_filled import (\n",
    "            export_user_hex_time_strict_filled,\n",
    "        )\n",
    "\n",
    "        if HEX_META is None:\n",
    "            raise FileNotFoundError(\n",
    "                \"HEX_META not found. Set env var POI_VISIT_HEX_META to your hex_meta_*.json path.\"\n",
    "            )\n",
    "\n",
    "        with step(\"export_user_hex_time_strict_filled\"):\n",
    "            export_user_hex_time_strict_filled(\n",
    "                city=CITY,\n",
    "                staypoints=[Path(p) for p in STAYPOINTS],\n",
    "                staypoints_format=\"auto\",  # or csv/parquet\n",
    "                uuid_table=UUID_TABLE,\n",
    "                hex_meta_path=HEX_META,\n",
    "                out_dir=OUT_DIR,\n",
    "                tmp_root=TMP_ROOT,\n",
    "                duckdb_temp_dir=DUCKDB_TEMP_DIR,\n",
    "                schema_map=SCHEMA_MAP,\n",
    "                output_hex_uid=True,\n",
    "                output_h3_id=True,\n",
    "                output_h3_int=False,\n",
    "                hex_uid_code=HEX_UID_CODE,\n",
    "                hex_uid_prefix=\"hex\",\n",
    "                filter_city_code=FILTER_CITY_CODE,\n",
    "                city_code_col=CITY_CODE_COL,\n",
    "                city_code_value=CITY_CODE_VALUE,\n",
    "                windows=[\"lunch\", \"dinner\"],\n",
    "                min_interval_minutes=5,\n",
    "                point_source_filter=True,\n",
    "                point_source_value=\"cell_appearance\",\n",
    "                drop_uuid_not_in_table=True,\n",
    "                timestamps_are_utc=True,\n",
    "                tz_offset_hours=8,\n",
    "                epoch_unit=\"ms\",\n",
    "                uid64_hash_method=\"xxh64\",\n",
    "                buckets=256,\n",
    "                batch_size=1_000_000,\n",
    "                log_every_batches=500,\n",
    "                overlap_rounding=\"floor\",\n",
    "                oob_mode=\"drop\",\n",
    "                threads=max(1, (os.cpu_count() or 8) - 1),\n",
    "                memory_limit=\"8GB\",\n",
    "                id_mode=\"uuid\",  # uuid|uid64|both\n",
    "                resume_stage2=RESUME_STAGE2,\n",
    "                keep_intermediate=KEEP_INTERMEDIATE,\n",
    "            )\n",
    "else:\n",
    "    print(\"RUN_EXPORT=False: skip export; visualize existing outputs.\")\n",
    "\n",
    "OUT_FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e457e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "assert OUT_FILE.exists(), f\"Missing output parquet: {OUT_FILE}\"\n",
    "assert QA_FILE.exists(), f\"Missing QA csv: {QA_FILE}\"\n",
    "\n",
    "qa = pd.read_csv(QA_FILE, encoding=\"utf-8-sig\")\n",
    "display(qa.T.head(120))\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.execute(f\"PRAGMA threads={max(1, (os.cpu_count() or 8) - 1)}\")\n",
    "\n",
    "# Light preview (avoid loading the full parquet into memory)\n",
    "df_head = con.execute(\"select * from read_parquet(?) limit 5\", [str(OUT_FILE)]).fetchdf()\n",
    "display(df_head)\n",
    "\n",
    "unit_col = UNIT_UID_COL\n",
    "overall_sql = f\"\"\"\n",
    "select\n",
    "  count(*) as n_rows,\n",
    "  count(distinct uuid) as n_users,\n",
    "  count(distinct {unit_col}) as n_units,\n",
    "  sum(tau_strict_min) as tau_strict_min_sum,\n",
    "  sum(tau_fill_min) as tau_fill_min_sum,\n",
    "  sum(tau_filled_min) as tau_filled_min_sum,\n",
    "  sum(case when tau_fill_min > 0 then 1 else 0 end) as n_rows_tau_fill_pos\n",
    "from read_parquet(?)\n",
    "\"\"\"\n",
    "\n",
    "overall = con.execute(overall_sql, [str(OUT_FILE)]).fetchdf()\n",
    "display(overall.T)\n",
    "\n",
    "by = con.execute(\n",
    "    '''\n",
    "    select\n",
    "      \"window\",\n",
    "      is_weekend,\n",
    "      count(*) as n_rows,\n",
    "      sum(tau_strict_min) as tau_strict_min_sum,\n",
    "      sum(tau_fill_min) as tau_fill_min_sum,\n",
    "      sum(tau_filled_min) as tau_filled_min_sum\n",
    "    from read_parquet(?)\n",
    "    group by 1,2\n",
    "    order by 1,2\n",
    "    ''',\n",
    "    [str(OUT_FILE)],\n",
    ").fetchdf()\n",
    "by[\"fill_share\"] = by[\"tau_fill_min_sum\"] / by[\"tau_filled_min_sum\"]\n",
    "display(by)\n",
    "\n",
    "# Top units by filled minutes\n",
    "top_units = con.execute(\n",
    "    f'''\n",
    "    select\n",
    "      {unit_col} as unit_uid,\n",
    "      sum(tau_filled_min) as tau_filled_min_sum\n",
    "    from read_parquet(?)\n",
    "    group by 1\n",
    "    order by 2 desc\n",
    "    limit 20\n",
    "    ''',\n",
    "    [str(OUT_FILE)],\n",
    ").fetchdf()\n",
    "display(top_units)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
