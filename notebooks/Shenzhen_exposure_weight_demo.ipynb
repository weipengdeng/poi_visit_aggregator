{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shenzhen exposure-weight demo (Colab / local)\n",
        "\n",
        "This notebook computes **user \u00d7 grid exposure weights** in lunch/dinner windows using `poi_visit_aggregator`.\n",
        "\n",
        "Outputs (per city): `user_grid_time_strict_filled_<city>.parquet` + `qa_summary_strict_filled_<city>.csv`.\n",
        "\n",
        "Tip: for stability/performance, put `tmp_root` and DuckDB temp on a local disk (Colab: `/tmp`, Windows: e.g. `C:\\\\temp`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Colab detection ---\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "\n",
        "def _find_repo_root(start: Path) -> Path:\n",
        "    p = start.resolve()\n",
        "    for parent in [p, *p.parents]:\n",
        "        if (parent / \"pyproject.toml\").exists() and (parent / \"poi_visit_aggregator\").exists():\n",
        "            return parent\n",
        "    return start.resolve()\n",
        "\n",
        "\n",
        "# --- Colab bootstrap (Drive + optional clone) ---\n",
        "# If you open this notebook directly in Colab (not opened from the repo),\n",
        "# keep CLONE_REPO_IN_COLAB=True to clone the repo into Drive and add it to sys.path.\n",
        "CLONE_REPO_IN_COLAB = True\n",
        "COLAB_TARGET_DIR = Path(\"/content/drive/MyDrive/Script/Module\")\n",
        "COLAB_REPO_PATH = COLAB_TARGET_DIR / \"poi_visit_aggregator\"\n",
        "\n",
        "if IN_COLAB and CLONE_REPO_IN_COLAB:\n",
        "    drive.mount(\"/content/drive\")\n",
        "    COLAB_TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    os.chdir(str(COLAB_TARGET_DIR))\n",
        "\n",
        "    if not COLAB_REPO_PATH.exists():\n",
        "        get_ipython().system(\"git clone https://github.com/weipengdeng/poi_visit_aggregator.git\")\n",
        "    else:\n",
        "        try:\n",
        "            get_ipython().system(f\"git -C {COLAB_REPO_PATH} pull\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if str(COLAB_REPO_PATH) in sys.path:\n",
        "        sys.path.remove(str(COLAB_REPO_PATH))\n",
        "    sys.path.insert(0, str(COLAB_REPO_PATH))\n",
        "    os.chdir(str(COLAB_REPO_PATH))\n",
        "\n",
        "if not IN_COLAB:\n",
        "    # VSCode/Jupyter sometimes sets CWD to `notebooks/`. Make repo imports work either way.\n",
        "    REPO_ROOT = _find_repo_root(Path.cwd())\n",
        "    os.chdir(str(REPO_ROOT))\n",
        "    if str(REPO_ROOT) not in sys.path:\n",
        "        sys.path.insert(0, str(REPO_ROOT))\n",
        "\n",
        "try:\n",
        "    import psutil  # type: ignore\n",
        "except Exception:\n",
        "    psutil = None\n",
        "\n",
        "\n",
        "def mem_gb():\n",
        "    \"\"\"Return (rss_gb, avail_gb, used_pct).\"\"\"\n",
        "    if psutil is None:\n",
        "        return (np.nan, np.nan, np.nan)\n",
        "    vm = psutil.virtual_memory()\n",
        "    rss = psutil.Process(os.getpid()).memory_info().rss\n",
        "    return (float(rss) / 1e9, float(vm.available) / 1e9, float(vm.percent))\n",
        "\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "pd.set_option(\"display.width\", 140)\n",
        "\n",
        "print(\"IN_COLAB:\", IN_COLAB)\n",
        "print(\"CWD:\", Path.cwd())\n",
        "rss_gb, avail_gb, used_pct = mem_gb()\n",
        "print(f\"Memory: RSS={rss_gb:.2f} GB | avail={avail_gb:.2f} GB | used={used_pct:.0f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install export deps (run once)\n",
        "# If you rerun the notebook often, you can comment this cell after the first run.\n",
        "#\n",
        "# Colab:\n",
        "# !pip -q install -e \".[export]\"\n",
        "#\n",
        "# Local (VSCode/Jupyter):\n",
        "# !{sys.executable} -m pip install -e \".[export]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "CITY = \"Shenzhen\"\n",
        "# City code that matches staypoints `c_code` (also used in grid_uid).\n",
        "GRID_UID_CODE = \"440300\"\n",
        "\n",
        "# --- Configure paths ---\n",
        "if IN_COLAB:\n",
        "    DRIVE_ROOT = Path(\"/content/drive/MyDrive\")\n",
        "    DATA_ROOT = DRIVE_ROOT / \"Project/202512_EFE\"\n",
        "\n",
        "    RUN_TAG = \"access_k10_popcentroid_v1\"\n",
        "    OUT_ROOT = DATA_ROOT / \"data\" / \"derived\" / \"accessibility\" / RUN_TAG\n",
        "    GRID_JSON_DIR = OUT_ROOT / \"grid_json\"\n",
        "\n",
        "    UUID_TABLE = DATA_ROOT / \"data/jike/uuid.csv\"  # .csv or .parquet\n",
        "    STAYPOINTS = [DATA_ROOT / \"data/jike/track.csv\"]\n",
        "    GRID_META = GRID_JSON_DIR / f\"grid_meta_{CITY}.json\"\n",
        "    OUT_DIR = OUT_ROOT / \"out/poi_visit_aggregator\"\n",
        "\n",
        "    # Recommended: keep temp files off Google Drive.\n",
        "    TMP_ROOT = Path(\"/tmp/poi_visit_aggregator_tmp\")\n",
        "    DUCKDB_TEMP_DIR = Path(\"/tmp/duckdb_tmp\")\n",
        "else:\n",
        "    # TODO: edit these paths to your local disk.\n",
        "    DATA_ROOT = Path(r\"D:\\\\Project\\\\202512_EFE\")\n",
        "    UUID_TABLE = DATA_ROOT / \"uuid.csv\"  # .csv or .parquet\n",
        "    STAYPOINTS = [DATA_ROOT / \"staypoints.csv\"]\n",
        "    GRID_META = DATA_ROOT / f\"grid_meta_{CITY}.json\"\n",
        "    OUT_DIR = DATA_ROOT / \"out/poi_visit_aggregator\"\n",
        "\n",
        "    # Put temp on a fast local disk (avoid OneDrive folders).\n",
        "    temp_dir = Path(os.environ.get(\"TEMP\", \".\"))\n",
        "    TMP_ROOT = Path(os.environ.get(\"POI_VISIT_TMP_ROOT\", str(temp_dir / \"poi_visit_aggregator_tmp\")))\n",
        "    DUCKDB_TEMP_DIR = Path(os.environ.get(\"POI_VISIT_DUCKDB_TMP\", str(temp_dir / \"duckdb_tmp\")))\n",
        "\n",
        "# If staypoints contain nationwide users, keep this on for speed.\n",
        "FILTER_CITY_CODE = True\n",
        "CITY_CODE_COL = \"c_code\"\n",
        "CITY_CODE_VALUE = GRID_UID_CODE\n",
        "\n",
        "OUT_CITY_DIR = OUT_DIR / CITY\n",
        "OUT_FILE = OUT_CITY_DIR / f\"user_grid_time_strict_filled_{CITY}.parquet\"\n",
        "QA_FILE = OUT_CITY_DIR / f\"qa_summary_strict_filled_{CITY}.csv\"\n",
        "\n",
        "OUT_CITY_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: map your column names if they differ.\n",
        "# Fill in only what you need.\n",
        "SCHEMA_MAP = {\n",
        "    \"staypoints\": {\n",
        "        # \"uuid\": \"uuid\",\n",
        "        # \"start_time\": \"start_ms\",\n",
        "        # \"end_time\": \"end_ms\",\n",
        "        # one of (x,y) or (lon,lat) or location\n",
        "        # \"lon\": \"lon\",\n",
        "        # \"lat\": \"lat\",\n",
        "        # \"source\": \"source\",\n",
        "    },\n",
        "    \"uuid_table\": {\n",
        "        # \"uuid\": \"uuid\",\n",
        "    },\n",
        "}\n",
        "SCHEMA_MAP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "\n",
        "try:\n",
        "    import psutil  # type: ignore\n",
        "except Exception:\n",
        "    psutil = None\n",
        "\n",
        "\n",
        "def rss_mb() -> float:\n",
        "    if psutil is None:\n",
        "        return float(\"nan\")\n",
        "    return psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def step(name: str):\n",
        "    t0 = time.perf_counter()\n",
        "    m0 = rss_mb()\n",
        "    print(f\"[START] {name} (RAM={m0:,.0f} MB)\")\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        dt = time.perf_counter() - t0\n",
        "        m1 = rss_mb()\n",
        "        print(f\"[ END ] {name} (dt={dt:,.1f}s, RAM={m1:,.0f} MB, \u0394={m1 - m0:,.0f} MB)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from poi_visit_aggregator.export_user_grid_time_strict_filled import (\n",
        "    export_user_grid_time_strict_filled,\n",
        ")\n",
        "\n",
        "RESUME_STAGE2 = False  # set True to reuse existing Stage-1 parts\n",
        "KEEP_INTERMEDIATE = True  # keep Stage-1 parts (useful for resume/debug)\n",
        "\n",
        "with step(\"export_user_grid_time_strict_filled\"):\n",
        "    export_user_grid_time_strict_filled(\n",
        "        city=CITY,\n",
        "        staypoints=[Path(p) for p in STAYPOINTS],\n",
        "        staypoints_format=\"auto\",  # or csv/parquet\n",
        "        uuid_table=UUID_TABLE,\n",
        "        grid_meta_path=GRID_META,\n",
        "        out_dir=OUT_DIR,\n",
        "        tmp_root=TMP_ROOT,\n",
        "        duckdb_temp_dir=DUCKDB_TEMP_DIR,\n",
        "        schema_map=SCHEMA_MAP,\n",
        "        output_grid_uid=True,\n",
        "        output_grid_id=False,\n",
        "        grid_uid_code=GRID_UID_CODE,\n",
        "        grid_uid_prefix=\"grid\",\n",
        "        grid_uid_order=\"col_row\",\n",
        "        filter_city_code=FILTER_CITY_CODE,\n",
        "        city_code_col=CITY_CODE_COL,\n",
        "        city_code_value=CITY_CODE_VALUE,\n",
        "        windows=[\"lunch\", \"dinner\"],\n",
        "        min_interval_minutes=5,\n",
        "        point_source_filter=True,\n",
        "        point_source_value=\"cell_appearance\",\n",
        "        drop_uuid_not_in_table=True,  # skip users not in UUID_TABLE\n",
        "        timestamps_are_utc=True,\n",
        "        tz_offset_hours=8,\n",
        "        epoch_unit=\"ms\",\n",
        "        coords_already_projected=False,\n",
        "        uid64_hash_method=\"xxh64\",  # faster if installed; else use sha256_64\n",
        "        buckets=256,\n",
        "        batch_size=1_000_000,\n",
        "        log_every_batches=500,\n",
        "        overlap_rounding=\"floor\",\n",
        "        oob_mode=\"drop\",\n",
        "        threads=max(1, (os.cpu_count() or 8) - 1),\n",
        "        memory_limit=\"8GB\",\n",
        "        id_mode=\"uuid\",  # uuid|uid64|both\n",
        "        resume_stage2=RESUME_STAGE2,\n",
        "        keep_intermediate=KEEP_INTERMEDIATE,\n",
        "    )\n",
        "\n",
        "OUT_FILE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(OUT_FILE)\n",
        "qa = pd.read_csv(QA_FILE)\n",
        "\n",
        "display(df.head())\n",
        "display(qa.T.head(50))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Weekday/weekend + per-week average examples\n",
        "import numpy as np\n",
        "\n",
        "DATE_START = \"2024-11-01\"  # change to your month\n",
        "DATE_END = \"2024-11-30\"\n",
        "dates = pd.date_range(DATE_START, DATE_END, freq=\"D\")\n",
        "n_days = len(dates)\n",
        "n_weekdays = int((dates.weekday < 5).sum())\n",
        "n_weekends = int((dates.weekday >= 5).sum())\n",
        "n_weeks = n_days / 7.0\n",
        "\n",
        "# Average minutes per (weekday/weekend) day\n",
        "df[\"tau_filled_min_per_day_type\"] = df[\"tau_filled_min\"] / np.where(df[\"is_weekend\"], n_weekends, n_weekdays)\n",
        "\n",
        "# Average minutes per week (over the period)\n",
        "df[\"tau_filled_min_per_week\"] = df[\"tau_filled_min\"] / n_weeks\n",
        "\n",
        "df[[\"window\", \"is_weekend\", \"tau_filled_min\", \"tau_filled_min_per_day_type\", \"tau_filled_min_per_week\"]].head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
